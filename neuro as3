import numpy as np

words = ['my', 'name', 'is', 'aly']
w_i = {word: i for i, word in enumerate(words)}
i_w = {i: word for word, i in word_to_idx.items()}
input_seq = np.array([w_i['my'], w_i['name'], w_i['is']])
target_idx = w_i['aly']
vocab_size = len(words)
embed_size = 8
hidden_size = 16
np.random.seed(42)
embedding = np.random.randn(vocab_size, embed_size)
Wxh = np.random.randn(embed_size, hidden_size) * 0.01
Whh = np.random.randn(hidden_size, hidden_size) * 0.01
Why = np.random.randn(hidden_size, vocab_size) * 0.01
bh = np.zeros((1, hidden_size))
by = np.zeros((1, vocab_size))
def softmax(x):
    exps = np.exp(x - np.max(x)) 
    return exps / np.sum(exps, axis=1, keepdims=True)
def cross_entropy(preds, label):
    return -np.log(preds[0, label])
learning_rate = 0.1
for ep in range(450):  # عدد الايبوك
    h = np.zeros((1, hidden_size))
    for i in input_seq:
        x = embedding[i].reshape(1, -1)
        h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)
    y = np.dot(h, Why) + by
    probs = softmax(y)
    loss = cross_entropy(probs, target_idx)
    dy = probs.copy()
    dy[0, target_idx] -= 1
    dWhy = np.dot(h.T, dy)
    dby = dy
    dh = np.dot(dy, Why.T) * (1 - h ** 2)
    dWxh = np.zeros_like(Wxh)
    dWhh = np.zeros_like(Whh)
    dbh = np.zeros_like(bh)
    prev_h = np.zeros((1, hidden_size))
    for i in input_seq:
        x = embedding[i].reshape(1, -1)
        dWxh += np.dot(x.T, dh)
        dWhh += np.dot(prev_h.T, dh)
        dbh += dh
        prev_h = np.tanh(np.dot(x, Wxh) + np.dot(prev_h, Whh) + bh)
    Wxh -= learning_rate * dWxh
    Whh -= learning_rate * dWhh
    Why -= learning_rate * dWhy
    bh -= learning_rate * dbh
    by -= learning_rate * dby

    if (ep+1) % 50 == 0:
        print("Epoch", ep+1, "Loss:", round(loss, 4))

h = np.zeros((1, hidden_size))
for i in input_seq:
    x = embedding[i].reshape(1, -1)
    h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)

y = np.dot(h, Why) + by
probs = softmax(y)
predicted_idx = np.argmax(probs)
print("Predicted word:", i_w[predicted_idx])
